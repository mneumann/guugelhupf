#
# Copyright (c) 2002 Michael Neumann <neumann@s-direktnet.de>
#
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without 
# modification, are permitted provided that the following conditions 
# are met:
# 1. Redistributions of source code must retain the above copyright 
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright 
#    notice, this list of conditions and the following disclaimer in the 
#    documentation and/or other materials provided with the distribution.
# 3. The name of the author may not be used to endorse or promote products
#    derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
# INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
# AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL
# THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
# ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#

require "util"

module GH


#
# Representation of a token.
# 
class Token
  attr_accessor :str, :pos
  def initialize(str, pos)
    @str, @pos = str, pos
  end
end
 

#
# Abstract superclass of all kinds of streams.
#
class Stream
  ##
  # returns:: nil on Eos
  #
  def next
    raise AbstractMethodException
  end

  def each(&b)
    while token = self.next
      b.call token  
    end
  end
end

#
# Abstract superclass of all streams returning Token objects.
#
class TokenStream < Stream
end

#
# Abstract superclass of all token filters.
#
class TokenFilter < TokenStream
  def initialize(inStream)
    @inStream = inStream
  end
end


class Tokenizer
  def initialize(stringOrReadable)
    @stringOrReadable = stringOrReadable
  end
end

# ---------------------------------------------------------------------------------------
# Concrete Implementations
# ---------------------------------------------------------------------------------------


class SimpleTokenizer < Tokenizer
  def initialize(stringOrReadable)
    super
    @next_callcc = nil
    @pos = 0
  end

  def next
    if @next_callcc.nil?
      @stringOrReadable.scan(/[A-Za-z]+/) do |token|
        callcc { | @next_callcc | return Token.new(token, @pos += 1) }
      end
      return nil
    else
      @next_callcc.call 
    end 
  end
end


class LowercaseTokenFilter < TokenFilter
  def next
    if token = @inStream.next
      token.str.downcase!
      token
    else
      nil
    end
  end
end

#
# Filters tokens through an external program.
#
class ExternalTokenFilter < TokenFilter
  def initialize(inStream, command, write_all_before_read=false) 
    super(inStream)
    @pipe = IO.popen(command, "w+") 
    @write_all_before_read = write_all_before_read

    @pipe.sync = true
    @write_thread = Thread.new {
      loop {
        @inStream.each do |token|
          put_token(token)
        end
      }
      @pipe.close_write
    }

    if not @write_all_before_read
      @write_thread.priority=-100_000 # TODO: stop?
    end
  end

  def next
    if @write_all_before_read
      get_token
    else
      if @write_thread.alive?
        @write_thread.priority=0  # TODO: wakeup?
        token = get_token 
        @write_thread.priority=-100_000 # TODO: stop?
        token
      else
        get_token 
      end
    end
  end

  private

  def put_token(token)
    # TODO: External format
    @pipe.puts "#{token.str} #{token.pos}"
  end

  def get_token
    # TODO: External format
    if token = @pipe.gets
      Token.new(*token.split)
    else
      nil
    end
  end
end



# multiset
# hash[token] = [ [token1, [pos, pos, ...]], [token2, [pos, pos, pos] ]
class Index
  attr_reader :index, :docId
  def initialize(docId)
    @index = {}
    @docId = docId
  end   

  def add(token)
    if @index.key? token.str
      list = @index[token.str]
      posting = list.assoc(token.str)
      if posting.nil?
        # create new posting
        posting = [token.str, [token.pos]]
        list << posting
      else
        # add position to existing posting
        posting[1] << token.pos
      end     
    else
      posting = [token.str, [token.pos]]
      @index[token.str] = [posting]
    end
  end
end

# TODO: find better names for class Posting and Word
class Posting
  attr_accessor :docId, :positions
end

class Word
  attr_accessor :word, :postings
end

#
# Stores multiple Index objects in one Hash.
# (the JoinedIndex is readonly).
#
class JoinedIndex
  def initialize
    @index = {} 
  end

  def addIndex(index)
    index.index.each_pair {|key, value| 
      if @index.key? key
        @index[]
      else
        [index.docId, 
      end
    }
  end
end

if __FILE__ == $0
  index = Index.new
  tokenizer = SimpleTokenizer.new(File.read("/tmp/a.doc"))
  tokenizer = LowercaseTokenFilter.new(tokenizer)
  #tokenizer = ExternalTokenFilter.new(tokenizer, "sort", true) 
  #tokenizer = ExternalTokenFilter.new(tokenizer, "grep l", true) 
  tokenizer.each do |token|
    index.add(token) 
    #p index.index
    #readline
  end
end

end # module GH
